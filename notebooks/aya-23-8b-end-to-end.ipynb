{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes peft accelerate datasets sentencepiece wandb python-dotenv wtpsplit langchain\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install wtpsplit==2.1.1\n",
    "!pip install syntok==1.4.4\n",
    "!pip install omegaconf\n",
    "!pip install wandb\n",
    "!pip install --upgrade transformers trl\n",
    "!pip install pandas numpy\n",
    "!pip install gdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/Reennon/gen-ai-nlp-lab-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd gen-ai-nlp-lab-1\n",
    "!ls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from huggingface_hub import login\n",
    "from transformers import PreTrainedTokenizerBase, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset\n",
    "from src.prompts.prompts import (NERPrompt1, NERPrompt2, NERPrompt3, NERPrompt4)\n",
    "from src.prompts.examples import (\n",
    "    FIFTEEN_SHOT_EXAMPLES_DICT,\n",
    "    ELEVEN_SHOT_EXAMPLES_DICT,\n",
    "    FIVE_SHOT_EXAMPLES_DICT,\n",
    "    THREE_SHOT_EXAMPLES_DICT,\n",
    "    ZERO_SHOT_EXAMPLES_DICT\n",
    ")\n",
    "from omegaconf import OmegaConf\n",
    "from google.colab import userdata\n",
    "import gdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "QUANTIZE_4BIT = False\n",
    "# device   = \"cuda:0\"\n",
    "device = \"cuda:0\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = OmegaConf.load(\"./params/aya_23_8b.yml\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "login(userdata.get('hf_key'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = \"CohereForAI/aya-23-8b\"\n",
    "quantization_config = None\n",
    "if QUANTIZE_4BIT:\n",
    "  quantization_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "  )\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "seq_length = parameters.baseline.max_new_tokens\n",
    "tokenizer.model_max_length = seq_length\n",
    "max_seq_length = seq_length\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    device_map=\"auto\",  # Automatically map to GPUs\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/uc?id=1kOOC_hgKBVK1HEew_DkfTdFLs-3cXkgT'\n",
    "output = './data/silver_test.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "silver_test_df = pd.read_csv(\"./data/silver_test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.prompts.prompts import BasePrompt\n",
    "\n",
    "class NERPrompt1(BasePrompt):\n",
    "    template: str = \"\"\"Твоє завдання – виділити всі сутності у наданому тексті за наведеними категоріями та вивести їх у форматі JSON-списку:\n",
    "\n",
    "Категорії сутностей (\"label\"):\n",
    "- ART: артефакт (створений людиною предмет)\n",
    "- DATE: дата (календарна дата, рік)\n",
    "- DOC: документ (назви документів)\n",
    "- JOB: посада (професійний титул, робоча позиція)\n",
    "- LOG: місце (географічні об’єкти, назви країн, міст, річок тощо)\n",
    "- MISC: різне (інші сутності, не підпадають під інші категорії)\n",
    "- MON: гроші (сума, валюта)\n",
    "- ORG: організація (установи, компанії, заклади)\n",
    "- PCT: відсоток (число у відсотках)\n",
    "- PERIOD: період (тривалість часу)\n",
    "- PERS: особа (людські імена, прізвища)\n",
    "- QUANT: кількість (числові значення)\n",
    "- TIME: час (конкретний момент доби)\n",
    "\n",
    "Формат відповіді: список об’єктів у JSON, кожен об’єкт має поля:\n",
    "\"label\" – категорія сутності\n",
    "\"text\" – фрагмент тексту сутності з оригінального тексту без змін\n",
    "\n",
    "Не виводь дублікати знайдених сутностей.\n",
    "Та не змінюй відмінків знайдених сутностей.\n",
    "Виводь сутності з вхідного тексту, а не прикладів.\n",
    "Перевір, чи справді знайдена сутність відповідає категорії сутностей.\n",
    "Якщо знайдена сутність не відповідає її категорії, то її не слід включати у відповідь.\n",
    "\n",
    "Нижче наведено приклади формату та стилю розпізнавання сутностей:\n",
    "{examples}\n",
    "\n",
    "ВХІДНИЙ ТЕКСТ:\n",
    "{text}\n",
    "\n",
    "ЗНАЙДЕНІ СУТНОСТІ:\"\"\"\n",
    "    input_variables: list[str] = [\"text\", \"examples\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_template: str = \"\"\"\n",
    "ПРИКЛАД ТЕКСТУ:\n",
    "{example_text}\n",
    "ЗНАЙДЕНІ СУТНОСТІ З ТЕКСТУ:\n",
    "{example_labels}\n",
    "\"\"\"\n",
    "\n",
    "def construct_prompt(\n",
    "    prompt_template: PromptTemplate,\n",
    "    few_shot_dict: dict[str, str],\n",
    "    text: str\n",
    ") -> str:\n",
    "    examples = \"\".join([\n",
    "            example_template.format(\n",
    "                example_text=example[\"text\"],\n",
    "                example_labels=example[\"labels\"]\n",
    "            ) for example in few_shot_dict\n",
    "        ])\n",
    "    prompt = prompt_template.format(\n",
    "        examples=examples,\n",
    "        text=text,\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "prompt = construct_prompt(\n",
    "    prompt_template=NERPrompt1().prompt_template,\n",
    "    few_shot_dict=ELEVEN_SHOT_EXAMPLES_DICT,\n",
    "    text=\"\"\n",
    ")\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer.encode(prompt)\n",
    "decoded_prompt = tokenizer.decode(tokenized_prompt)\n",
    "prompt_len = len(tokenizer.tokenize(prompt))\n",
    "print(decoded_prompt), prompt_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_message_format(prompts):\n",
    "  messages = []\n",
    "\n",
    "  for p in prompts:\n",
    "    messages.append(\n",
    "        [{\"role\": \"user\", \"content\": p}]\n",
    "      )\n",
    "\n",
    "  return messages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "out_labels = {}\n",
    "max_seq_length = prompt_len + seq_length\n",
    "\n",
    "batch_size_max = 4\n",
    "\n",
    "for id in tqdm(silver_test_df.id.unique(), desc=\"Text progress\"):\n",
    "    inputs: list[str] = silver_test_df.loc[silver_test_df.loc[:, \"id\"] == id, \"text\"].to_list()\n",
    "    inputs: list[str] = [construct_prompt(\n",
    "        prompt_template=NERPrompt1().prompt_template,\n",
    "        few_shot_dict=THREE_SHOT_EXAMPLES_DICT,\n",
    "        text=input\n",
    "    ) for input in inputs]\n",
    "    print(\"inputs len\")\n",
    "    print([len(tokenizer.encode(input)) for input in inputs])\n",
    "    print()\n",
    "    inputs: list[dict[str, str]] = get_message_format(inputs)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        inputs,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    if len(input_ids) < batch_size_max:\n",
    "      input_ids = input_ids.to(model.device)\n",
    "      #input_ids = input_ids.to(model.device)\n",
    "      prompt_padded_len = len(input_ids[0])\n",
    "      # Generate corrections\n",
    "      # Check if the model is wrapped in DataParallel\n",
    "      gen_tokens = model.generate(\n",
    "              input_ids,\n",
    "              temperature=parameters.baseline.temperature,\n",
    "              top_p=parameters.baseline.top_p,\n",
    "              top_k=parameters.baseline.top_k,\n",
    "              max_new_tokens=seq_length,\n",
    "              do_sample=True,\n",
    "      )\n",
    "\n",
    "      gen_tokens = [\n",
    "          gt[prompt_padded_len:] for gt in gen_tokens\n",
    "      ]\n",
    "      outputs: list[dict] = tokenizer.batch_decode(\n",
    "          gen_tokens,\n",
    "          skip_special_tokens=True\n",
    "      )\n",
    "    else:\n",
    "      from torch.utils.data import DataLoader\n",
    "      outputs: list[dict] = []\n",
    "      dataloader = DataLoader(input_ids, batch_size=batch_size_max, shuffle=False)\n",
    "\n",
    "      for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        prompt_padded_len = len(batch[0])\n",
    "        gen_tokens = model.generate(\n",
    "            batch,\n",
    "            temperature=parameters.baseline.temperature,\n",
    "            top_p=parameters.baseline.top_p,\n",
    "            top_k=parameters.baseline.top_k,\n",
    "            max_new_tokens=seq_length,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        gen_tokens = [gt[prompt_padded_len:] for gt in gen_tokens]\n",
    "        outputs.extend(\n",
    "            tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"output len\")\n",
    "    print(len(tokenizer.encode(str(outputs))))\n",
    "    print(\"output\")\n",
    "    print(id, outputs)\n",
    "\n",
    "    out_labels[id] = outputs\n",
    "\n",
    "print(f\"All texts extracted!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_incomplete_entries(data: str) -> str:\n",
    "    pattern = r\"\\{[^}]*\\}\"  # Matches everything enclosed in {}\n",
    "\n",
    "    # Find all matches of complete entries\n",
    "    complete_entries = re.findall(pattern, data)\n",
    "\n",
    "    # Join back into a single cleaned string\n",
    "    cleaned_data = ', '.join(complete_entries)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def filter_unique_dicts(dict_list):\n",
    "    seen = set()\n",
    "    unique_dicts = []\n",
    "\n",
    "    for d in dict_list:\n",
    "        # Convert dictionary to a frozenset of its items (immutable and hashable)\n",
    "        items = frozenset(d.items())\n",
    "        if items not in seen:\n",
    "            seen.add(items)\n",
    "            unique_dicts.append(d)\n",
    "\n",
    "    return unique_dicts\n",
    "\n",
    "def join_split_dicts(entities):\n",
    "  some_list = []\n",
    "  for entity_list in entities:\n",
    "    some_list.extend(entity_list)\n",
    "\n",
    "  return some_list\n",
    "\n",
    "def postprocess(entities):\n",
    "  try:\n",
    "    result_dict_list = []\n",
    "    for e in entities:\n",
    "      e = clean_incomplete_entries(e)\n",
    "      e = ast.literal_eval(e)\n",
    "      e = list(e)\n",
    "      e = filter_unique_dicts(e)\n",
    "      result_dict_list.extend(e)\n",
    "\n",
    "    result_dict_list = filter_unique_dicts(result_dict_list)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"skipping due to error, \", e)\n",
    "    result_dict_list = []\n",
    "\n",
    "  return result_dict_list\n",
    "\n",
    "def filter_dicts_by_text(dict_list, original_text):\n",
    "    return [d for d in dict_list if 'text' in d and d['text'] in original_text]\n",
    "\n",
    "silver_test_predictions_df = pd.DataFrame.from_dict([out_labels]).T\n",
    "silver_test_predictions_df.columns = [\"entities\"]\n",
    "silver_test_predictions_df.index.name = \"id\"\n",
    "silver_test_predictions_df = silver_test_predictions_df.reset_index()\n",
    "silver_test_predictions_df.loc[:, \"entities\"] = \\\n",
    "  silver_test_predictions_df.loc[:, \"entities\"].apply(lambda e: postprocess(e))\n",
    "silver_test_df_concatenated = (\n",
    "    silver_test_df.groupby(\"id\", as_index=False)[\"text\"].apply(\" \".join).reset_index()\n",
    ")\n",
    "silver_test_predictions_df = pd.merge(\n",
    "    silver_test_predictions_df,\n",
    "    silver_test_df_concatenated.loc[:, [\"text\", \"id\"]],\n",
    "    how=\"left\",\n",
    "    on=\"id\"\n",
    ")\n",
    "silver_test_predictions_df.loc[:, \"entities\"] = \\\n",
    "  silver_test_predictions_df.apply(lambda e: filter_dicts_by_text(e[\"entities\"], e[\"text\"]), axis=1)\n",
    "silver_test_predictions_df.loc[:, \"entities_dumps\"] = \\\n",
    "  silver_test_predictions_df.loc[:, \"entities\"].apply(lambda e: json.dumps(e))\n",
    "silver_test_predictions_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "silver_test_predictions_df.to_csv(f\"./data/submission_first_run.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submissions_df = pd.read_csv(\"./data/submission_first_run.csv\")\n",
    "\n",
    "not_scored_ids = submissions_df.loc[submissions_df.loc[:, \"entities\"] == \"[]\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_labels = {}\n",
    "\n",
    "batch_size_max = 4\n",
    "\n",
    "for id in tqdm(not_scored_ids.id.unique(), desc=\"Text progress\"):\n",
    "    inputs: list[str] = silver_test_df.loc[silver_test_df.loc[:, \"id\"] == id, \"text\"].to_list()\n",
    "    inputs: list[str] = [construct_prompt(\n",
    "        prompt_template=NERPrompt1().prompt_template,\n",
    "        few_shot_dict=THREE_SHOT_EXAMPLES_DICT,\n",
    "        text=input\n",
    "    ) for input in inputs]\n",
    "    print(\"inputs len\")\n",
    "    print([len(tokenizer.encode(input)) for input in inputs])\n",
    "    print()\n",
    "    inputs: list[dict[str, str]] = get_message_format(inputs)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        inputs,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    if len(input_ids) < batch_size_max:\n",
    "      input_ids = input_ids.to(model.device)\n",
    "      #input_ids = input_ids.to(model.device)\n",
    "      prompt_padded_len = len(input_ids[0])\n",
    "      # Generate corrections\n",
    "      # Check if the model is wrapped in DataParallel\n",
    "      gen_tokens = model.generate(\n",
    "              input_ids,\n",
    "              temperature=parameters.baseline.temperature,\n",
    "              top_p=parameters.baseline.top_p,\n",
    "              top_k=parameters.baseline.top_k,\n",
    "              max_new_tokens=seq_length,\n",
    "              do_sample=True,\n",
    "      )\n",
    "\n",
    "      gen_tokens = [\n",
    "          gt[prompt_padded_len:] for gt in gen_tokens\n",
    "      ]\n",
    "      outputs: list[dict] = tokenizer.batch_decode(\n",
    "          gen_tokens,\n",
    "          skip_special_tokens=True\n",
    "      )\n",
    "    else:\n",
    "      from torch.utils.data import DataLoader\n",
    "      outputs: list[dict] = []\n",
    "      dataloader = DataLoader(input_ids, batch_size=batch_size_max, shuffle=False)\n",
    "\n",
    "      for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        prompt_padded_len = len(batch[0])\n",
    "        gen_tokens = model.generate(\n",
    "            batch,\n",
    "            temperature=parameters.baseline.temperature,\n",
    "            top_p=parameters.baseline.top_p,\n",
    "            top_k=parameters.baseline.top_k,\n",
    "            max_new_tokens=seq_length,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        gen_tokens = [gt[prompt_padded_len:] for gt in gen_tokens]\n",
    "        outputs.extend(\n",
    "            tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    print(\"output len\")\n",
    "    print(len(tokenizer.encode(str(outputs))))\n",
    "    print(\"output\")\n",
    "    print(id, outputs)\n",
    "\n",
    "    out_labels[id] = outputs\n",
    "\n",
    "print(f\"All non scored texts has been scored!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "silver_test_predictions_df = pd.DataFrame.from_dict([out_labels]).T\n",
    "silver_test_predictions_df.columns = [\"entities\"]\n",
    "silver_test_predictions_df.index.name = \"id\"\n",
    "silver_test_predictions_df = silver_test_predictions_df.reset_index()\n",
    "silver_test_predictions_df.loc[:, \"entities\"] = \\\n",
    "  silver_test_predictions_df.loc[:, \"entities\"].apply(lambda e: postprocess(e))\n",
    "silver_test_df_concatenated = (\n",
    "    silver_test_df.groupby(\"id\", as_index=False)[\"text\"].apply(\" \".join).reset_index()\n",
    ")\n",
    "silver_test_predictions_df = pd.merge(\n",
    "    silver_test_predictions_df,\n",
    "    silver_test_df_concatenated.loc[:, [\"text\", \"id\"]],\n",
    "    how=\"left\",\n",
    "    on=\"id\"\n",
    ")\n",
    "silver_test_predictions_df.loc[:, \"entities\"] = \\\n",
    "  silver_test_predictions_df.apply(lambda e: filter_dicts_by_text(e[\"entities\"], e[\"text\"]), axis=1)\n",
    "silver_test_predictions_df.loc[:, \"entities_dumps\"] = \\\n",
    "  silver_test_predictions_df.loc[:, \"entities\"].apply(lambda e: json.dumps(e))\n",
    "silver_test_predictions_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "silver_test_predictions_df.to_csv(f\"./data/submission_second_run.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submissions_second_run_df = pd.read_csv(f\"./data/submission_second_run.csv\")\n",
    "submissions_second_run_df = submissions_second_run_df.reset_index(drop=True)\n",
    "submissions_second_run_df = submissions_second_run_df.drop_duplicates(subset=\"id\")\n",
    "submissions_second_run_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submissions_df = pd.read_csv(\"../data/submission_merged.csv\")\n",
    "print(len(submissions_df.loc[submissions_df.loc[:, 'entities'] == \"[]\"]))\n",
    "submissions_df = pd.concat([\n",
    "    submissions_df.loc[~submissions_df.loc[:, \"id\"].isin(submissions_second_run_df.id)],\n",
    "    submissions_second_run_df,\n",
    "])\n",
    "print(len(submissions_df.loc[submissions_df.loc[:, 'entities'] == \"[]\"]))\n",
    "submissions_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submissions_df = submissions_df.loc[:, [\"id\", \"entities_dumps\"]]\n",
    "submissions_df.columns = [\"id\", \"entities\"]\n",
    "submissions_df.to_csv(\"../data/submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
